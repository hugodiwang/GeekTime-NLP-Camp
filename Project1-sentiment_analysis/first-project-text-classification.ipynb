{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # This is the first project 'sentiment analysis of IMDB' of the GeekTime's NLP Camp. I will record some possible modifications and improvement ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and check the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dwang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# here we use the punkt to do word tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenizer = word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to use the combination of word embedding/ sentence embedding and character-level embedding. The torchtext is not a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data are from the IMDB dataset, which includes movie reviews\n",
    "# here we use the torchtext to build the data set and do some preprocessing. \n",
    "# we set the seed and cuda(if used) to repeat the results\n",
    "\n",
    "#!pip install torchtext\n",
    "import torch\n",
    "from torchtext import data, datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = tokenizer, include_lengths = True) # TEXT defines the format of the text data\n",
    "LABEL = data.LabelField(dtype = torch.float) # LABEL defines the format of the label data\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# if we later try other local dataset we can use this\n",
    "# train, val, test = data.TabularDataset.splits(\n",
    "#         path='./data/', train='train.tsv',\n",
    "#         validation='val.tsv', test='test.tsv', format='tsv',\n",
    "#         fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size 20000\n",
      "valid data size 5000\n",
      "test data size 25000\n"
     ]
    }
   ],
   "source": [
    "# Let's check the size of each data sets\n",
    "print(\"train data size\", len(train_data))\n",
    "print(\"valid data size\", len(valid_data))\n",
    "print(\"test data size\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, the train and test data are splite equally by IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a large word embedding 6B/300, which is a good choice. But since the code runs on my laptop without GPU, I add a hyperparameter MAX_VOCAB_SIZE, which influences the performance greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after that, let's build the vocabulary\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, vectors = \"glove.6B.300d\",\n",
    "                unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prefer a small batch_size at first and increase the large batch_size later to stabilize the perforamnce.\n",
    "NOTES:\n",
    "When our memory is not enoughr for batch_size = 1 /2, we can do the following:\n",
    "1, we can use half precision, like 16 bits flaot.\n",
    "2, we can use distributed computation.(Consider DeepSpeed)\n",
    "3, we can do gradient accumulation. Because of the error, please do not use more than steps' accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we learn with batch, we need to build the iterator\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "(train_data, valid_data, test_data), batch_size = BATCH_SIZE, sort_within_batch = True, device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we need to fix the parameters of the embedding at first with the pretrained paramters. After training sveral epoches, we can train its parameters and the lstm parameters together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seqlength is an important paramter. Please do not use the maximum word length as the seqLength. If it is too large, \n",
    "the convergence speed will be too slow. We need more padding which is harmful for the final precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters initialization, random initialization with normal distribution is a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step is to define the neural network structure\n",
    "import torch.nn as nn\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional,\n",
    "                          dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "        output, out_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:],hidden[-1,:,:]), dim=1))\n",
    "\n",
    "        return self.fc(hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dwang\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:57: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS,\n",
    "           BIDIRECTIONAL, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to pay attention to the padding, unknown items, since they do not need to take part in \n",
    "# the training process. Also we torchtext provide us the pretrained text vector\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WarmUp:\n",
    "We can use a small learning rate at the first to ensure a good initialization and then increase it for the main training process. When approaching convergence, we need to decrease the learning rate to approach the local optimals. \n",
    "\n",
    "It is recommended to use multiple loss iterms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before start the training process, we need to define the related parts\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        print(\"epoch_loss\", loss.item(), \"epoch_acc\",acc.item() )\n",
    "    return epoch_loss / len(iteratore), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            loss = criterionion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iteratore), epoch_acc / len(iterator)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not recommended to only save the best validation model. Because\n",
    "1, the near best model is useful for later hand tuning tasks\n",
    "2, we need to save learning rate/ optimizer and other info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_loss 0.6766699552536011 epoch_acc 0.4375\n",
      "epoch_loss 0.654873251914978 epoch_acc 0.53125\n",
      "epoch_loss 0.6881387233734131 epoch_acc 0.53125\n",
      "epoch_loss 0.6945408582687378 epoch_acc 0.5625\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion) # To validate model at the end of each epoch is also serious offence. \n",
    "    \n",
    "    if valid_loss < best_valid_loss: # This is serious offence here. It is a VERY bad idea to only save the best model. When you want to babysit the training process, it is usually a bad idea to start from the best model. \n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt') # The estimator state should also be saved (as well as scheduler if available)\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
